{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "integrated-regular",
   "metadata": {},
   "source": [
    "### 기본 Library 선언 및 Tensorflow 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alleged-lebanon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0-rc0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "tf.random.set_seed(777)  # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-harris",
   "metadata": {},
   "source": [
    "# Softmax_cross_entropy_with_logits\n",
    "06-1Softmax Classifier.ipynb에서 구현한 hypothesis, cost function을 좀더 편리한 방법으로 정의해볼까 한다.<br>  \n",
    "tf.nn.softmax_cross_entropy_with_logits_v2() 은 deprecate 되었다...<br>\n",
    "tf.keras.losses.categorical_crossentropy(y_true=Y, y_pred=logits, from_logits=True) 로 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-thickness",
   "metadata": {},
   "source": [
    "## Softmax function\n",
    "\n",
    "linear한 결과 값인 XW + b를 0~1사이의 값으로 변환\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "S(y_{i}) & = \\frac{e^{y_{i}}}{\\sum_{j} e^{y_{i}}}  \\\\\\\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-glance",
   "metadata": {},
   "source": [
    "## Cross entropy cost / loss\n",
    "\n",
    "\\begin{align}\n",
    "D(S,L)& = -{\\sum_{i}L_{i}log(S_{i}) }  \\\\\\\\\\\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "L_{ce}& = \\frac{1}{N}{\\sum_{i} D(S(WX_{i}+b)L_{i}})  \\\\\\\\\\\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-methodology",
   "metadata": {},
   "source": [
    "이전 코드에서 구현한 hypothesis 는 다음과 같다.\n",
    "``` python\n",
    "logits = tf.matmul(X, W) + b\n",
    "hypothesis = tf.nn.softmax(logit)\n",
    "```\n",
    "그리고 이를 활용한 cost function은 다음과 같다.\n",
    "```python\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "```\n",
    "수식을 그래로 구현하면 되지만 좀 더 간소화시켜서 구현할 수 있는 것을 Tensorflow는 제공하고 있다.\n",
    "\n",
    "```python\n",
    "# Cross entropy cost/loss\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y_one_hot) # deprecate되었다...\n",
    "cost_i = tf.keras.losses.categorical_crossentropy(y_true=Y, y_pred=logits, from_logits=True) # 해당 방법으로 수정\n",
    "\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "```\n",
    "\n",
    "Y_one_hot은 제공되는 Y(실제 값)가 one_hot 속성을 갖기 때문에 이렇게 이름을 붙였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-dynamics",
   "metadata": {},
   "source": [
    "# Softmax Zoo_classifier\n",
    "\n",
    "* Softmax를 사용하여 Zoo 데이터를 활용하여 분류를 진행."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-truck",
   "metadata": {},
   "source": [
    "## Data load\n",
    "\n",
    "![shape.jpg](자료img/animal_data_shape.jpg) \n",
    "python numpy 슬라이싱 참고 https://076923.github.io/posts/Python-numpy-5/  \n",
    "xy[:,:1] # [: , :] [ 행 , 열]  \n",
    "xy[:1] # [행]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "structural-standing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16) (101, 1)\n",
      "[[0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt(\"data/data-04-zoo.csv\", delimiter=\",\", dtype=np.float32)\n",
    "x_data = xy[: , 0:-1]\n",
    "y_data = xy[:,[-1]]# 2차원으로 데이터를 가져옴\n",
    "\n",
    "print(x_data.shape, y_data.shape)\n",
    "print(y_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-polls",
   "metadata": {},
   "source": [
    "## tf.one_hot and reshape\n",
    "y_data를 살펴보면 0~6으로 7종류로 동물을 구분하고 있다. 하지만 우리가 필요한 one_hot으로 되어있는 값이다.  \n",
    "그래서 tensorflow는 one_hot으로 바꿔주는 tf.one_hot()을 제공한다.  \n",
    "\n",
    "예제를 통해 설명해보자면...\n",
    "```python\n",
    "nb_classes = 7\n",
    "Y = tf.placeholder(tf.int32, [None, 1]) # 0 ~ 6, shape=(?, 1)\n",
    "```\n",
    "일단 y_data는 shape = (?, 1)임을 알 수가 있다.  \n",
    "```python\n",
    "Y_one_hot = tf.one_hot(Y, nb_classes) # one hot shape=(?, 1, 7)\n",
    "```\n",
    "tf.one_hot()의 인자로 데이터, 클래스 개수를 넘겨준다.  \n",
    "하지만 이때 문제가 발생하는데 one_hot의 다음과 같은 속성 때문이다.<br><br>\n",
    "\n",
    "**<center>If the input indices is rank N, the output will have rank N+1. The new axis is<br>    \n",
    "created at dimension axis (default: the new axis is appended at the end).</center>**<br><br>\n",
    "쉽게 설명하자면 차원을 1차원 더 크게 만들어 준다는 의미이다. 즉, 1차원을 넣으면 2차원을 반환\n",
    "\n",
    "위의 animal 데이터를 예시로 들자면 y_data는 다음과 같은 형태이다.\n",
    "<center>[[0.],[3.]]<br>shape = (?, 1)</center><br>\n",
    "근데 이 데이터를 one_hot을 해버리면 차원을 더하면서 다음과 같은 형태로 바뀐다.  \n",
    "<center>[[[1. 0. 0. 0. 0. 0. 0.]], [[0. 0. 0. 1. 0. 0. 0.]]]<br>shape = (?, 1, 7)</center><br>\n",
    "하지만 이러한 형태는 우리가 원하는 형태가 아니다. 그래서 이를 해결할 방법이 tf.reshape()이다.<br>\n",
    "\n",
    "```python\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes]) # shape=(?, 7)\n",
    "```\n",
    "첫 번째 인자로 변환할 배열을 넣어준다.<br>\n",
    "두 번째 인자는 변환할 형태를 정의해주면 된다. [ -1:모든 부분,  클래스 개수]<br><br>\n",
    "그래서 reshpe을 거친 데이터는 다음과 같은 모양이 될것이다.\n",
    "<center>[[1. 0. 0. 0. 0. 0. 0.]], [[0. 0. 0. 1. 0. 0. 0.]]<br>shape = (?, 7)</center><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-illness",
   "metadata": {},
   "source": [
    "### tf.one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "hindu-literature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 7) \n",
      " tf.Tensor(\n",
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]], shape=(5, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "nb_classes = 7 # 0 ~ 6\n",
    "Y_one_hot = tf.one_hot(y_data.astype(np.int32), nb_classes)\n",
    "print(Y_one_hot.shape, \"\\n\",Y_one_hot[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-michael",
   "metadata": {},
   "source": [
    "### tf.reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "faced-armenia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 7) \n",
      " tf.Tensor(\n",
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]], shape=(5, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes]) # shape=(?, 7)\n",
    "print(Y_one_hot.shape, \"\\n\", Y_one_hot[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-onion",
   "metadata": {},
   "source": [
    "추후 one_hot encoding만 따로 정리하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-gallery",
   "metadata": {},
   "source": [
    "## Softmax Zoo_classifier 전체 코드\n",
    "위에서 y_data를 가져올 때 y_data = xy[:,[-1]] 와 같이 진행하였다. 이렇게 가져오면 2차원으로 데이터를 가져오는 꼴이고 나중에 reshape해줘야하는 불편 사항이 존재하였다. (모두의 딥러닝 시즌1에선 이런식으로 진행)<br><br>\n",
    "이 부분은 수정하여 전체 코드를 진행하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "toxic-conversion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16) (101,)\n",
      "[0. 0. 3. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt(\"data/data-04-zoo.csv\", delimiter=\",\", dtype=np.float32)\n",
    "x_data = xy[: , 0:-1]\n",
    "y_data = xy[:,-1] # 1차원으로 데이터를 가져옴\n",
    "\n",
    "print(x_data.shape, y_data.shape)\n",
    "print(y_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "constitutional-stretch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16) (101, 7)\n",
      "tf.Tensor(\n",
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]], shape=(5, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "nb_classes = 7  # 0 ~ 6\n",
    "\n",
    "# Make Y data as onehot shape\n",
    "Y_one_hot = tf.one_hot(y_data.astype(np.int32), nb_classes)\n",
    "\n",
    "print(x_data.shape, Y_one_hot.shape)\n",
    "print(Y_one_hot[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cultural-botswana",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weight and bias setting\n",
    "W = tf.Variable(tf.random.normal((16, nb_classes)), name='weight')\n",
    "b = tf.Variable(tf.random.normal((nb_classes,)), name='bias')\n",
    "variables = [W, b]\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "def logit_fn(X): # scroe : 실수 값\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "def hypothesis(X):\n",
    "    return tf.nn.softmax(logit_fn(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-whale",
   "metadata": {},
   "source": [
    "##### 왜 굳이 logit_fn(), hypothesis()를 구분해서 정의하였을까?\n",
    "logit은 categorical_crossentropy()에서 활용하기 위해 정의하였다고 납득할 수 있다. 그렇다면 hypothesis는?<br>\n",
    "이는 이후에 학습을 하면서 정확도를 맞추기 위한 도구로 사용이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_fn(X, Y):\n",
    "    logits = logit_fn(X)\n",
    "    cost_i = tf.keras.losses.categorical_crossentropy(y_true=Y, y_pred=logits, from_logits=True)\n",
    "    cost = tf.reduce_mean(cost_i) \n",
    "    return cost\n",
    "\n",
    "def cost_fn2(X, Y):\n",
    "    logits = hypothesis(X)\n",
    "    cost_i = -tf.reduce_sum(Y* tf.math.log(logits), axis=1)\n",
    "    cost = tf.reduce_mean(cost_i) \n",
    "    return cost\n",
    "\n",
    "def grad_fn(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = cost_fn(X, Y)\n",
    "        loss2 =  cost_fn2(X, Y)\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-particular",
   "metadata": {},
   "source": [
    "**위 코드의 cost_fn, cost_fn2는 같은 동작을 함.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(X, Y):\n",
    "    pred = tf.argmax(hypothesis(X), 1)\n",
    "    correct_prediction = tf.equal(pred, tf.argmax(Y, 1)) # Y 와 예측 값을 비교\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-denial",
   "metadata": {},
   "source": [
    "prediction 함수에서 위에서 정의했던 hypothesis를 사용하여 정확도를 비교한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-sapphire",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "danish-helmet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 1 Loss: 0.10128264874219894, Acc: 0.9900990128517151\n",
      "epochs: 100 Loss: 0.09306582808494568, Acc: 1.0\n",
      "epochs: 200 Loss: 0.08603505790233612, Acc: 1.0\n",
      "epochs: 300 Loss: 0.08000823110342026, Acc: 1.0\n",
      "epochs: 400 Loss: 0.07478483021259308, Acc: 1.0\n",
      "epochs: 500 Loss: 0.07021408528089523, Acc: 1.0\n",
      "epochs: 600 Loss: 0.0661807656288147, Acc: 1.0\n",
      "epochs: 700 Loss: 0.06259509176015854, Acc: 1.0\n",
      "epochs: 800 Loss: 0.059386301785707474, Acc: 1.0\n",
      "epochs: 900 Loss: 0.056497715413570404, Acc: 1.0\n",
      "epochs: 1000 Loss: 0.053883470594882965, Acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "def fit(X, Y, epochs=1000, verbose=100):\n",
    "    optimizer =  tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        grads = grad_fn(X, Y)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        if (i==0) | ((i+1)%verbose==0):\n",
    "            acc = prediction(X, Y).numpy()\n",
    "            loss = cost_fn(X, Y).numpy() \n",
    "            print('epochs: {} Loss: {}, Acc: {}'.format(i+1, loss, acc))\n",
    "\n",
    "fit(x_data, Y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-moral",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
