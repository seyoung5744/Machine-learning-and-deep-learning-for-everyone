{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sexual-stable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-induction",
   "metadata": {},
   "source": [
    "# Multi-variable linear regression\n",
    "Predicting exam score - regression using three inputs (x1, x2, x3)\n",
    "\n",
    "x1 (quiz 1) | x2 (quiz 2) | x3 (mid 1) | Y (final)\n",
    "---- | ---- | ----| ----\n",
    "73 | 80 | 75 | 152\n",
    "93 | 88 | 93 | 185\n",
    "89 | 91 | 90 | 180\n",
    "96 | 98 | 100 | 196\n",
    "73 | 66 | 70 | 142\n",
    "\n",
    "\n",
    "Test Scores for General Psychology ( https://goo.gl/g2T8Kp )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-apollo",
   "metadata": {},
   "source": [
    "# Matrix multiplication\n",
    "\n",
    "## dot product(=scalar product, 내적)\n",
    "<img src=\"https://www.mathsisfun.com/algebra/images/matrix-multiply-a.svg\" >\n",
    "\n",
    "\n",
    "https://www.mathsisfun.com/algebra/matrix-multiplying.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-housing",
   "metadata": {},
   "source": [
    "# Multi-feature regression\n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "$$ H(x) = w x + b $$\n",
    "\n",
    "$$ H(x_1, x_2, x_3) = w_1 x_1 + w_2 x_2 + w_3 x_3 + b $$  \n",
    "각 변수에 대한 w 값이 필요함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-concrete",
   "metadata": {},
   "source": [
    "# Hypothesis using matrix\n",
    "\n",
    "$$ H(x_1, x_2, x_3) = \\underline{w_1 x_1 + w_2 x_2 + w_3 x_3} + b $$\n",
    "\n",
    "$$ w_1 x_1 + w_2 x_2 + w_3 x_3 $$ \n",
    "\n",
    "$$ \\begin{pmatrix} w_{ 1 } & w_{ 2 } & w_{ 3 } \\end{pmatrix}\\cdot \\begin{pmatrix} x_{ 1 } \\\\ x_{ 2 } \\\\ x_{ 3 } \\end{pmatrix} $$\n",
    "\n",
    "$$ WX $$ (W, X 는 matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-interest",
   "metadata": {},
   "source": [
    "# Hypothesis without b\n",
    "\n",
    "$$ H(x_1, x_2, x_3) = w_1 x_1 + w_2 x_2 + w_3 x_3 + b$$\n",
    "\n",
    "$$ = b + w_1 x_1 + w_2 x_2 + w_3 x_3 $$\n",
    "\n",
    "$$ = \\begin{pmatrix} b & x_{ 1 } & x_{ 2 } & x_{ 3 } \\end{pmatrix}\\cdot \\begin{pmatrix} 1 \\\\ w_{ 1 } \\\\ w_{ 2 } \\\\ w_{ 3 } \\end{pmatrix} $$\n",
    "\n",
    "$$ = XW $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-idaho",
   "metadata": {},
   "source": [
    "# Hypothesis using matrix \n",
    "\n",
    "### Many x instances\n",
    "\n",
    "$$ \\begin{pmatrix} x_{ 11 } & x_{ 12 } & x_{ 13 } \\\\ x_{ 21 } & x_{ 22 } & x_{ 23 } \\\\ x_{ 31 } & x_{ 32 } & x_{ 33 }\\\\ x_{ 41 } & x_{ 42 } & x_{ 43 }\\\\ x_{ 51 } & x_{ 52 } & x_{ 53 }\\end{pmatrix} \\cdot \\begin{pmatrix} w_{ 1 } \\\\ w_{ 2 } \\\\ w_{ 3 } \\end{pmatrix}=\\begin{pmatrix} x_{ 11 }w_{ 1 }+x_{ 12 }w_{ 2 }+x_{ 13 }w_{ 3 } \\\\ x_{ 21 }w_{ 1 }+x_{ 22 }w_{ 2 }+x_{ 23 }w_{ 3 }\\\\ x_{ 31 }w_{ 1 }+x_{ 32 }w_{ 2 }+x_{ 33 }w_{ 3 } \\\\ x_{ 41 }w_{ 1 }+x_{ 42 }w_{ 2 }+x_{ 43 }w_{ 3 } \\\\ x_{ 51 }w_{ 1 }+x_{ 52 }w_{ 2 }+x_{ 53 }w_{ 3 } \\end{pmatrix} $$\n",
    "\n",
    "$$ [5, 3] \\cdot [3, 1] = [5, 1] $$\n",
    "\n",
    "$$ H(X) = XW $$\n",
    "\n",
    "5는 데이터(instance)의 수, 3은 변수(feature)의 수, 1은 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-spending",
   "metadata": {},
   "source": [
    "# Hypothesis using matrix (n output)\n",
    "\n",
    "$$ [n, 3] \\cdot [?, ?] = [n, 2] $$\n",
    "\n",
    "$$ H(X) = XW $$\n",
    "\n",
    "* n은 데이터(instance)의 개수, 2는 결과 값의 개수로 주어진다.\n",
    "* 이때, W [?, ?] ⇒ [3, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-norway",
   "metadata": {},
   "source": [
    "# WX vs XW\n",
    "\n",
    "### Theory (Lecture) :\n",
    " $$ H(x) = Wx + b  $$\n",
    "\n",
    "### TensorFlow (Implementation) :\n",
    "\n",
    "$$ H(X) = XW $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-cancellation",
   "metadata": {},
   "source": [
    "# Simple Example (2 variables)\n",
    "\n",
    "x1 | x2 | y\n",
    "---- | ---- | ----\n",
    "1  |  0  |  1\n",
    "0  |  2  |  2\n",
    "3  |  0  |  3\n",
    "0  |  4  |  4\n",
    "5  |  0  |  5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "durable-liability",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "breathing-sculpture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch |       cost |         W1 |         W2 |          b\n",
      "    0 | 156.990295 |     6.9780 |     4.2124 |  -5.968578\n",
      "   50 |  41.852776 |     4.7775 |     3.7767 |  -6.553596\n",
      "  100 |  16.238819 |     3.7377 |     3.5222 |  -6.778894\n",
      "  150 |  10.350182 |     3.2397 |     3.3644 |  -6.833573\n",
      "  200 |   8.829857 |     2.9943 |     3.2595 |  -6.807239\n",
      "  250 |   8.288828 |     2.8666 |     3.1845 |  -6.742532\n",
      "  300 |   7.974858 |     2.7937 |     3.1268 |  -6.659857\n",
      "  350 |   7.719106 |     2.7462 |     3.0795 |  -6.569052\n",
      "  400 |   7.482807 |     2.7107 |     3.0386 |  -6.474894\n",
      "  450 |   7.256645 |     2.6808 |     3.0019 |  -6.379736\n",
      "  500 |   7.038133 |     2.6536 |     2.9679 |  -6.284741\n",
      "  550 |   6.826455 |     2.6279 |     2.9357 |  -6.190496\n",
      "  600 |   6.621229 |     2.6030 |     2.9049 |  -6.097299\n",
      "  650 |   6.422207 |     2.5787 |     2.8750 |  -6.005295\n",
      "  700 |   6.229178 |     2.5548 |     2.8460 |  -5.914558\n",
      "  750 |   6.041956 |     2.5312 |     2.8177 |  -5.825120\n",
      "  800 |   5.860364 |     2.5081 |     2.7899 |  -5.736990\n",
      "  850 |   5.684230 |     2.4852 |     2.7626 |  -5.650168\n",
      "  900 |   5.513391 |     2.4628 |     2.7358 |  -5.564643\n",
      "  950 |   5.347687 |     2.4406 |     2.7095 |  -5.480402\n",
      " 1000 |   5.186961 |     2.4188 |     2.6835 |  -5.397429\n"
     ]
    }
   ],
   "source": [
    "x1_data = [1, 0, 3, 0, 5]\n",
    "x2_data = [0, 2, 0, 4, 0]\n",
    "y_data  = [1, 2, 3, 4, 5]\n",
    "\n",
    "W1 = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
    "W2 = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
    "b  = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
    "\n",
    "learning_rate = tf.Variable(0.001)\n",
    "\n",
    "print(\"{:5s} | {:>10.6s} | {:>10.4s} | {:>10.4s} | {:>10.6s}\".format( \"epoch\",\"cost\",\"W1\",\"W2\",\"b\"))\n",
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = W1 * x1_data + W2 * x2_data + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "        \n",
    "    W1_grad, W2_grad, b_grad = tape.gradient(cost, [W1, W2, b])\n",
    "    W1.assign_sub(learning_rate * W1_grad)\n",
    "    W2.assign_sub(learning_rate * W2_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n",
    "          i, cost.numpy(), W1.numpy()[0], W2.numpy()[0], b.numpy()[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-prediction",
   "metadata": {},
   "source": [
    "# Simple Example (2 variables with Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "associate-absence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch |       cost |         W1 |         W2 |          b\n",
      "    0 |   1.638827 |     0.3503 |     0.6276 |   0.781139\n",
      "   50 |   0.473447 |     0.5729 |     0.6699 |   0.836680\n",
      "  100 |   0.213363 |     0.6784 |     0.6949 |   0.856284\n",
      "  150 |   0.152576 |     0.7294 |     0.7106 |   0.858900\n",
      "  200 |   0.135874 |     0.7549 |     0.7213 |   0.853497\n",
      "  250 |   0.129053 |     0.7685 |     0.7293 |   0.844330\n",
      "  300 |   0.124540 |     0.7765 |     0.7356 |   0.833432\n",
      "  350 |   0.120644 |     0.7820 |     0.7409 |   0.821779\n",
      "  400 |   0.116978 |     0.7862 |     0.7457 |   0.809842\n",
      "  450 |   0.113451 |     0.7899 |     0.7501 |   0.797852\n",
      "  500 |   0.110038 |     0.7932 |     0.7542 |   0.785920\n",
      "  550 |   0.106729 |     0.7964 |     0.7581 |   0.774104\n",
      "  600 |   0.103521 |     0.7996 |     0.7619 |   0.762433\n",
      "  650 |   0.100410 |     0.8026 |     0.7656 |   0.750917\n",
      "  700 |   0.097392 |     0.8056 |     0.7692 |   0.739564\n",
      "  750 |   0.094465 |     0.8085 |     0.7727 |   0.728376\n",
      "  800 |   0.091626 |     0.8114 |     0.7762 |   0.717354\n",
      "  850 |   0.088872 |     0.8143 |     0.7796 |   0.706496\n",
      "  900 |   0.086201 |     0.8171 |     0.7830 |   0.695801\n",
      "  950 |   0.083610 |     0.8199 |     0.7863 |   0.685267\n",
      " 1000 |   0.081097 |     0.8226 |     0.7895 |   0.674891\n"
     ]
    }
   ],
   "source": [
    "x_data = [\n",
    "    [1., 0., 3., 0., 5.], # x1\n",
    "    [0., 2., 0., 4., 0.]  # x2\n",
    "]\n",
    "y_data  = [1, 2, 3, 4, 5]\n",
    "\n",
    "W = tf.Variable(tf.random.uniform((1, 2), -1.0, 1.0))\n",
    "b = tf.Variable(tf.random.uniform((1,), -1.0, 1.0))\n",
    "\n",
    "learning_rate = tf.Variable(0.001)\n",
    "\n",
    "print(\"{:5s} | {:>10.6s} | {:>10.4s} | {:>10.4s} | {:>10.6s}\".format( \"epoch\",\"cost\",\"W1\",\"W2\",\"b\"))\n",
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = tf.matmul(W, x_data) + b # (1, 2) * (2, 5) = (1, 5)\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "\n",
    "        W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "        W.assign_sub(learning_rate * W_grad)\n",
    "        b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n",
    "            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], b.numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-decade",
   "metadata": {},
   "source": [
    "# Hypothesis without b\n",
    "- 해당 코드의 tf.keras.optimizers.SGD 같은 경우 추후 설명\n",
    "- matrix, gradient에 치중해서 코드를 보기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "straight-alaska",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |   7.240298 |     0.0610 |     0.3030 |     0.0054\n",
      "   50 |   2.202286 |     0.2372 |     0.6339 |     0.3182\n",
      "  100 |   0.757683 |     0.3289 |     0.7811 |     0.5150\n",
      "  150 |   0.297664 |     0.3767 |     0.8453 |     0.6400\n",
      "  200 |   0.136139 |     0.4009 |     0.8726 |     0.7203\n",
      "  250 |   0.074774 |     0.4122 |     0.8840 |     0.7724\n",
      "  300 |   0.049964 |     0.4163 |     0.8887 |     0.8065\n",
      "  350 |   0.039313 |     0.4163 |     0.8908 |     0.8292\n",
      "  400 |   0.034360 |     0.4138 |     0.8921 |     0.8445\n",
      "  450 |   0.031759 |     0.4100 |     0.8931 |     0.8550\n",
      "  500 |   0.030149 |     0.4053 |     0.8942 |     0.8625\n",
      "  550 |   0.028968 |     0.4001 |     0.8954 |     0.8681\n",
      "  600 |   0.027982 |     0.3947 |     0.8966 |     0.8723\n",
      "  650 |   0.027093 |     0.3891 |     0.8980 |     0.8757\n",
      "  700 |   0.026258 |     0.3834 |     0.8994 |     0.8785\n",
      "  750 |   0.025461 |     0.3778 |     0.9008 |     0.8809\n",
      "  800 |   0.024692 |     0.3722 |     0.9022 |     0.8831\n",
      "  850 |   0.023948 |     0.3666 |     0.9037 |     0.8851\n",
      "  900 |   0.023228 |     0.3611 |     0.9051 |     0.8870\n",
      "  950 |   0.022530 |     0.3557 |     0.9065 |     0.8888\n",
      " 1000 |   0.021852 |     0.3503 |     0.9079 |     0.8906\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 앞의 코드에서 bias(b)를 행렬에 추가\n",
    "x_data = [\n",
    "    [1., 1., 1., 1., 1.], # bias(b)\n",
    "    [1., 0., 3., 0., 5.], \n",
    "    [0., 2., 0., 4., 0.]\n",
    "]\n",
    "y_data  = [1, 2, 3, 4, 5]\n",
    "\n",
    "W = tf.Variable(tf.random.uniform((1, 3), -1.0, 1.0)) # [1, 3]으로 변경하고, b 삭제\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "\n",
    "print(\"{:5s} | {:>10.6s} | {:>10.4s} | {:>10.4s} | {:>10.6s}\".format( \"epoch\",\"cost\",\"W1\",\"W2\",\"W3\"))\n",
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = tf.matmul(W, x_data) # b가 없다\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "\n",
    "    grads = tape.gradient(cost, [W])\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads,[W]))\n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.4f}\".format(\n",
    "            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], W.numpy()[0][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-poster",
   "metadata": {},
   "source": [
    "# Custom Gradient\n",
    "* tf.train.GradientDescentOptimizer(): optimizer\n",
    "* optimizer.apply_gradients(): update\n",
    "\n",
    "- 해당 코드도 우선 참고 사항으로 알아두기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fifty-throw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch | cost\n",
      "    0 | 101.103889\n",
      "   50 |   0.003188\n",
      "  100 |   0.002180\n",
      "  150 |   0.001490\n",
      "  200 |   0.001019\n",
      "  250 |   0.000697\n",
      "  300 |   0.000476\n",
      "  350 |   0.000326\n",
      "  400 |   0.000223\n",
      "  450 |   0.000152\n",
      "  500 |   0.000104\n",
      "  550 |   0.000071\n",
      "  600 |   0.000049\n",
      "  650 |   0.000033\n",
      "  700 |   0.000023\n",
      "  750 |   0.000016\n",
      "  800 |   0.000011\n",
      "  850 |   0.000007\n",
      "  900 |   0.000005\n",
      "  950 |   0.000003\n",
      " 1000 |   0.000002\n"
     ]
    }
   ],
   "source": [
    "# Multi-variable linear regression (1)\n",
    "\n",
    "X = tf.constant([[1., 2.], \n",
    "                 [3., 4.]])\n",
    "y = tf.constant([[1.5], [3.5]])\n",
    "\n",
    "W = tf.Variable(tf.random.normal((2, 1)))\n",
    "b = tf.Variable(tf.random.normal((1,)))\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "n_epoch = 1000+1\n",
    "print(\"epoch | cost\")\n",
    "for i in range(n_epoch):\n",
    "    # Use tf.GradientTape() to record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = tf.matmul(X, W) + b\n",
    "        cost = tf.reduce_mean(tf.square(y_pred - y))\n",
    "\n",
    "    # calculates the gradients of the loss\n",
    "    grads = tape.gradient(cost, [W, b])\n",
    "    \n",
    "    # updates parameters (W and b)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, [W, b]))\n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:10.6f}\".format(i, cost.numpy()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-series",
   "metadata": {},
   "source": [
    "# 아래 코드부터가 모두의 딥러닝 강의 부분에 포함됐던것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-extent",
   "metadata": {},
   "source": [
    "# Predicting exam score\n",
    "regression using three inputs (x1, x2, x3)\n",
    "\n",
    "x1 (quiz 1) | x2 (quiz 2) | x3 (mid 1) | Y (final)\n",
    "---- | ---- | ----| ----\n",
    "73 | 80 | 75 | 152\n",
    "93 | 88 | 93 | 185\n",
    "89 | 91 | 90 | 180\n",
    "96 | 98 | 100 | 196\n",
    "73 | 66 | 70 | 142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)  # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-beijing",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "\n",
    "# data and label\n",
    "x1 = [ 73.,  93.,  89.,  96.,  73.]\n",
    "x2 = [ 80.,  88.,  91.,  98.,  66.]\n",
    "x3 = [ 75.,  93.,  90., 100.,  70.]\n",
    "Y  = [152., 185., 180., 196., 142.] # label, true value\n",
    "\n",
    "# weights\n",
    "w1 = tf.Variable(10.)\n",
    "w2 = tf.Variable(10.)\n",
    "w3 = tf.Variable(10.)\n",
    "b  = tf.Variable(10.)\n",
    "\n",
    "hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b        \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "orange-aaron",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 | 5793889.5000\n",
      "   50 |   64291.1562\n",
      "  100 |     715.2903\n",
      "  150 |       9.8461\n",
      "  200 |       2.0152\n",
      "  250 |       1.9252\n",
      "  300 |       1.9210\n",
      "  350 |       1.9177\n",
      "  400 |       1.9145\n",
      "  450 |       1.9114\n",
      "  500 |       1.9081\n",
      "  550 |       1.9050\n",
      "  600 |       1.9018\n",
      "  650 |       1.8986\n",
      "  700 |       1.8955\n",
      "  750 |       1.8923\n",
      "  800 |       1.8892\n",
      "  850 |       1.8861\n",
      "  900 |       1.8829\n",
      "  950 |       1.8798\n",
      " 1000 |       1.8767\n"
     ]
    }
   ],
   "source": [
    "# data and label\n",
    "x1 = [ 73.,  93.,  89.,  96.,  73.]\n",
    "x2 = [ 80.,  88.,  91.,  98.,  66.]\n",
    "x3 = [ 75.,  93.,  90., 100.,  70.]\n",
    "Y  = [152., 185., 180., 196., 142.]\n",
    "\n",
    "# weights\n",
    "w1 = tf.Variable(10.)\n",
    "w2 = tf.Variable(10.)\n",
    "w3 = tf.Variable(10.)\n",
    "b  = tf.Variable(10.)\n",
    "\n",
    "learning_rate = 0.000001\n",
    "\n",
    "for i in range(1000+1):\n",
    "    # tf.GradientTape() to record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "    # calculates the gradients of the cost\n",
    "    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n",
    "    \n",
    "    # update w1,w2,w3 and b\n",
    "    w1.assign_sub(learning_rate * w1_grad)\n",
    "    w2.assign_sub(learning_rate * w2_grad)\n",
    "    w3.assign_sub(learning_rate * w3_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-campus",
   "metadata": {},
   "source": [
    "## Multi-variable linear regression (1)\n",
    "*  random  초기화: tf.random_normal()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "subjective-credit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch | cost\n",
      "    0 |   40574.8281\n",
      "   50 |      13.1069\n",
      "  100 |      12.7682\n",
      "  150 |      12.4384\n",
      "  200 |      12.1175\n",
      "  250 |      11.8051\n",
      "  300 |      11.5011\n",
      "  350 |      11.2052\n",
      "  400 |      10.9172\n",
      "  450 |      10.6368\n",
      "  500 |      10.3640\n",
      "  550 |      10.0984\n",
      "  600 |       9.8399\n",
      "  650 |       9.5884\n",
      "  700 |       9.3435\n",
      "  750 |       9.1052\n",
      "  800 |       8.8732\n",
      "  850 |       8.6474\n",
      "  900 |       8.4276\n",
      "  950 |       8.2137\n",
      " 1000 |       8.0055\n"
     ]
    }
   ],
   "source": [
    "# data and label\n",
    "x1 = [ 73.,  93.,  89.,  96.,  73.]\n",
    "x2 = [ 80.,  88.,  91.,  98.,  66.]\n",
    "x3 = [ 75.,  93.,  90., 100.,  70.]\n",
    "Y  = [152., 185., 180., 196., 142.] # label, true value\n",
    "\n",
    "# weights - 초기값은 1로 줬지만 이전에도 말했다시피 이 초기값은 아무 값이나 줘도 되고 보통 랜덤값을 할당한다.\n",
    "w1 = tf.Variable(tf.random.normal([1]))\n",
    "w2 = tf.Variable(tf.random.normal([1]))\n",
    "w3 = tf.Variable(tf.random.normal([1]))\n",
    "b  = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "# 위에서 변수가 3개이므로 weight 값도 3개가 필요하다.\n",
    "\n",
    "learning_rate = 0.00001\n",
    "\n",
    "print(\"epoch | cost\")\n",
    "\n",
    "# 학습은 Gradient descent 이용 - weight 값을 지속적으로 업데이트\n",
    "for i in range(1000 + 1):\n",
    "    # tf.GradientTape() to record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape: # with문 안에 있는 변수들에 대한 정보를 Tape에 기록\n",
    "        hypothesis = w1 * x1 + w2 * x2 + w3 * x3 + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "        \n",
    "    # calculates the gradient of the cost\n",
    "    # tape.gradient를 통해 cost함수에 대한 w1,w2,w3,b 변수 4개의 기울기 값을 구한다.\n",
    "    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1,w2,w3,b]) \n",
    "    \n",
    "    # update w1, w2, w3 and b\n",
    "    # w1, w2, w3, b에 위에서 구한 lr * 기울기 값을 업데이트\n",
    "    w1.assign_sub(learning_rate * w1_grad)\n",
    "    w2.assign_sub(learning_rate * w2_grad)\n",
    "    w3.assign_sub(learning_rate * w3_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-alaska",
   "metadata": {},
   "source": [
    "위의 결과에서 보이듯이 cost가 매우 큰 값이였다가 일정 수준이 되면 감소하는 빈도가 줄어드는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-perspective",
   "metadata": {},
   "source": [
    "## Multi-variable linear regression (2)\n",
    "\n",
    "* Matrix 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "palestinian-battery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch | cost\n",
      "    0 |   213.9912\n",
      "  100 |    15.7712\n",
      "  200 |    15.6649\n",
      "  300 |    15.5834\n",
      "  400 |    15.5023\n",
      "  500 |    15.4216\n",
      "  600 |    15.3414\n",
      "  700 |    15.2616\n",
      "  800 |    15.1822\n",
      "  900 |    15.1034\n",
      " 1000 |    15.0248\n",
      " 1100 |    14.9469\n",
      " 1200 |    14.8693\n",
      " 1300 |    14.7920\n",
      " 1400 |    14.7153\n",
      " 1500 |    14.6388\n",
      " 1600 |    14.5628\n",
      " 1700 |    14.4872\n",
      " 1800 |    14.4120\n",
      " 1900 |    14.3373\n",
      " 2000 |    14.2629\n"
     ]
    }
   ],
   "source": [
    "# Matrix를 이용해서 데이터 준비\n",
    "data = np.array([\n",
    "    # X1,   X2,    X3,   y\n",
    "    [ 73.,  80.,  75., 152. ],\n",
    "    [ 93.,  88.,  93., 185. ],\n",
    "    [ 89.,  91.,  90., 180. ],\n",
    "    [ 96.,  98., 100., 196. ],\n",
    "    [ 73.,  66.,  70., 142. ]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# slice data\n",
    "# slice를 이용해 X, Y 값을 추출\n",
    "# slice에서 ,를 기준으로 앞 부분은 행, 뒷 부분은 열을 의미한다.\n",
    "X = data[:, :-1]\n",
    "y = data[:, [-1]] # label, answer\n",
    "\n",
    "W = tf.Variable(tf.random.normal((3, 1)))\n",
    "b = tf.Variable(tf.random.normal((1,)))\n",
    "\n",
    "learning_rate = 0.000001\n",
    "\n",
    "# hypothesis, prediction function\n",
    "def predict(X):\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "print(\"epoch | cost\")\n",
    "\n",
    "n_epochs = 2000\n",
    "for i in range(n_epochs+1):\n",
    "    # tf.GradientTape() to record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = tf.reduce_mean((tf.square(predict(X) - y)))\n",
    "\n",
    "    # calculates the gradients of the loss\n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "\n",
    "    # updates parameters (W and b)\n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"{:5} | {:10.4f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-receiver",
   "metadata": {},
   "source": [
    "### slice 설명\n",
    "```python\n",
    "X = data[:, :-1]\n",
    "y = data[:, [-1]]\n",
    "```\n",
    "**slice에서 ,를 기준으로 앞 부분은 행, 뒷 부분은 열을 의미한다.**<br>\n",
    "\" : \" 을 기준으로 앞뒤로 아무것도 안 써있기 때문에 처음부터 끝까지 라는 의미<br>\n",
    "y에서 [-1]은 마지막 컬럼만을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-tucson",
   "metadata": {},
   "source": [
    "### W 설명\n",
    "```python\n",
    "W = tf.Variable(tf.random.normal((3, 1)))\n",
    "```\n",
    "변수가 3개 w의 low는 3개 필요. 출력 값은 y하나이므로 w의 column은 1개 필요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-cigarette",
   "metadata": {},
   "source": [
    "### 가설 함수, 예측 함수 설명\n",
    "```python\n",
    "# hypothesis, prediction function\n",
    "def predict(X):\n",
    "    return tf.matmul(X, W) + b\n",
    "```\n",
    "tf.matmul => matrix multiplication\n",
    "b(bias)는 이후에 생략도 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-polls",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-burke",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-bangladesh",
   "metadata": {},
   "source": [
    "# With Matrix\n",
    "Matrix를 사용했을 때와 안했을 때를 비교."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-prison",
   "metadata": {},
   "source": [
    "### matrix를 사용하지 않을 때"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-essay",
   "metadata": {},
   "source": [
    "\n",
    "$$ \\begin{pmatrix} x_{ 1 } & x_{ 2 } & x_{ 3 } \\end{pmatrix}\\cdot \\begin{pmatrix} w_{ 1 } \\\\ w_{ 2 } \\\\ w_{ 3 } \\end{pmatrix} \n",
    "= x_1 w_1 + x_2 w_2 + x_3 w_3 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-public",
   "metadata": {},
   "source": [
    "```python\n",
    "# initialize w\n",
    "w1 = tf.Variable(tf.random.normal([1]))\n",
    "w2 = tf.Variable(tf.random.normal([1]))\n",
    "w3 = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "# hypothesis, prediction function\n",
    "w1 * x1 +  w2 * x2 + w3 * x3 + b      \n",
    "\n",
    "# update w1,w2,w3\n",
    "w1.assign_sub(learning_rate * w1_grad)\n",
    "w2.assign_sub(learning_rate * w2_grad)\n",
    "w3.assign_sub(learning_rate * w3_grad)\n",
    "```\n",
    "\n",
    "matrix를 사용하지 않는다면 변수를 따로따로 기술해주고 이 변수의 개수만큼 w가 필요하게 된다.<br>\n",
    "그렇게되면 w1 * x1 +  w2 * x2 + w3 * x3 와 같은 수식을 따로따로 써줘야하고 변수가 100개 이상이면 매우 귀찮아진다.\n",
    "또한 w를 업데이트할 때도 변수의 개수만큼 assign_sub를 따로따로 기술해줘야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-adapter",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-jenny",
   "metadata": {},
   "source": [
    "### matrix를 사용할 때"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-vision",
   "metadata": {},
   "source": [
    "$$ H(X) = XW $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-waters",
   "metadata": {},
   "source": [
    "```python\n",
    "# initialize w\n",
    "W = tf.Variable(tf.random.normal((3, 1)))\n",
    "\n",
    "# hypothesis, prediction function\n",
    "tf.matmul(X, W) + b\n",
    "    \n",
    "# update parameters (w and b)\n",
    "W.assign_sub(learning_rate * W_grad)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-damages",
   "metadata": {},
   "source": [
    "matrix를 사용하게 되면 데이터를 기술하고 weight 정의와 연산을 한 줄로 기술이 가능하다.<br>\n",
    "또한 weight를 업데이트하는 것도 weight가 matrix이므로 한 줄로 기술이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "pharmaceutical-attack",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8225908 , 0.78949606]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "blond-district",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.3974295], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "magnetic-ability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       "array([[145.62068],\n",
       "       [188.54604],\n",
       "       [178.70964],\n",
       "       [195.95045],\n",
       "       [146.04646]], dtype=float32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 우리의 모델, 우리의 예측, 우리의 가설\n",
    "tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-nancy",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "damaged-berry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[152.0, 185.0, 180.0, 196.0, 142.0]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y # labels, 실제값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "electoral-bookmark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[145.62068],\n",
       "       [188.54604],\n",
       "       [178.70964],\n",
       "       [195.95045],\n",
       "       [146.04646]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X).numpy() # prediction, 예측값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "associate-pledge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[179.197  ],\n",
       "       [166.21233]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 새로운 데이터에 대한 예측\n",
    "\n",
    "predict([[ 89.,  95.,  92.],[ 84.,  92.,  85.]]).numpy() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
