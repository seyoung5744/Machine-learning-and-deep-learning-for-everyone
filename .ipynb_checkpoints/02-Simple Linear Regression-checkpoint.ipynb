{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "iraqi-salvation",
   "metadata": {},
   "source": [
    "# 02 - Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "otherwise-dictionary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-baptist",
   "metadata": {},
   "source": [
    "# Hypothesis and Cost\n",
    "### Hypothesis\n",
    "$$ H(x) = Wx + b $$\n",
    "\n",
    "### Cost\n",
    "$$ cost(W)=\\frac { 1 }{ m } \\sum _{i=1}^{m}{ { (W{ x }^{ i }-y^{ i } })^{ 2 } }  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-expense",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1, 2, 3, 4, 5] # Input\n",
    "y_data = [1, 2, 3, 4, 5] # Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x_data, y_data, 'o')\n",
    "plt.ylim(0, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-anderson",
   "metadata": {},
   "source": [
    "## Hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-divorce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1, 2, 3, 4, 5]\n",
    "y_data = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-while",
   "metadata": {},
   "source": [
    "위의 준비된 데이터의 input, output이 동일하게 한 이유는 x, y 값이 같으면 위의 H(x) = Wx + b의 W, b의 값을 예측해볼 수 있기 때문이다. <br>\n",
    "입력과 출력값이 같으려면 W = 1, b = 0가 되어야 한다고 우리는 예측할 수 있다.<br>\n",
    "이렇게 예측을 해두고 나중에 실제로 W = 1, b = 0이 실제로 나오는지 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(2.9)\n",
    "b = tf.Variable(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-beast",
   "metadata": {},
   "source": [
    "W = 2.9, b = 0.5로 초기값을 지정하였다. 해당 초기값은 임의의 값을 지정할 수 있고 실제로 이후에 진행할 때 보면 W, b의 값은 초기에 렌덤 값으로 부여한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-canada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothesis = W * x + b\n",
    "hypothesis = W * x_data + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "W.numpy(), b.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-sapphire",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-married",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_data, hypothesis.numpy(), 'r-')\n",
    "plt.plot(x_data, y_data, 'o')\n",
    "plt.ylim(0, 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-physiology",
   "metadata": {},
   "source": [
    "## Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(hypothesis - y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-thinking",
   "metadata": {},
   "source": [
    "위의 cost function은 error (hypothesis - y_data) 제곱(tf.square)의 평균(tf.reduce_mean)으로 우리는 정의하였다.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-membrane",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = [1., 2., 3., 4.]\n",
    "tf.reduce_mean(v) # 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-burke",
   "metadata": {},
   "source": [
    "여기서 reduce는 MapReduce의 reduce라고 하는데 어려운 내용이라서 강의에선 뒤로 skip <br>\n",
    "\n",
    "일단 간단하게 말해서 redece는 차원이 줄어든다. 차원 즉 랭크가 줄어들면서 mean을 구한다고 해서 앞에 reduce라는 말이 붙었다고 한다.<br>\n",
    "\n",
    "위의 코드에서 v의 차원. 즉, 랭크는 1차원이다. 이 1차원을 reduce_mean() 해서 나온 결과는 2.5. 즉, 0차원이 된것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "strong-fraction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=9>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# square은 넘겨 받은 값을 제곱하는 함수\n",
    "tf.square(3) # 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-fossil",
   "metadata": {},
   "source": [
    "이렇게 tenserflow 함수를 이용해서 cost 함수를 정의해보았다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-house",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-austria",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-dividend",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-prayer",
   "metadata": {},
   "source": [
    "## Gradient descent(경사 하강법)\n",
    "\n",
    "cost가 최소가 되도록 하는 W, b를 찾는 Minimization 알고리즘이 여러 가지 존재한다. <br>\n",
    "그리고 그 중에서 가장 유명한 것이 Gradient descent이다. <br>\n",
    "결국 이 Gredient descent는 cost를 minimize하게 하는 W, b를 찾는 알고리즘인 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate initialize -4)\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Gradient descent\n",
    "with tf.GradientTape() as tape: # -1)\n",
    "    hypothesis = W * x_data + b # -2)\n",
    "    cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "    \n",
    "W_grad, b_grad = tape.gradient(cost, [W, b]) # -2)\n",
    "\n",
    "W.assign_sub(learning_rate * W_grad) # -3)\n",
    "b.assign_sub(learning_rate * b_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-banking",
   "metadata": {},
   "source": [
    "1) <br>\n",
    "Gredient descent는 tensorflow의 GradientTape을 갖고 구현한다. GradientTape은 보통 with 구문이랑 많이 사용한다. <br>\n",
    "with 구문안에 있는 코드는 변수들의 변화. 즉,정보를 Tape에 기록한다.<br>\n",
    "그리고 이 Tape에 기록된 변수에 대한 정보를 Tape의 gradient 메소드를 호출하여 경사도 값. 즉, 미분 값을 구한다.<br>\n",
    "\n",
    "2)<br>\n",
    "이 gradient 함수는 이 함수에 대해서 변수들에 대한 개별 미분 값. 즉 기울기 값을 구해서 튜플로 반환을 한다.<br>\n",
    "여기서 미분값은 기울기를 의미한다.<br>\n",
    "\n",
    "3)<br>\n",
    "그리고 나서 W, b 값을 업데이트 해준다. <br>\n",
    "assign_sub 함수는 경사도를 계산한 값을 다시 할당해주는 즉 파이썬에서 -= 과 같은 역할을 해준다.<br>\n",
    "\n",
    "4) \n",
    "learning_rate 는 이 gradient 값을 얼마만큼 반영할 것인지를 결정한다.<br>\n",
    "주로 이 learning_rate는 굉장히 작은 값을 사용한다. 보통은 0.001, 0.00001과 같은 매우 작은 값을 사용한다. \n",
    "\n",
    "\n",
    "**여기까지가 한 걸음 경사를 내려왔다고 생각하면 될것이다. W, b가 한번 업데이트 된 것이다.<br>\n",
    "이 작업을 한번만 실행하는 것이 아닌 여러 차례 거쳐서 W, b를 업데이트하게 된다.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-lexington",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-longitude",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-attachment",
   "metadata": {},
   "source": [
    "# 여러 번 반복\n",
    "# Parameter(W,b) Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-director",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(2.9)\n",
    "b = tf.Variable(0.5)\n",
    "\n",
    "for i in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = W * x_data + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    if i % 10 == 0:\n",
    "        print(\"{:5}|{:10.4f}|{:10.4f}|{:10.6f}\".format(i, W.numpy(), b.numpy(), cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-garage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Data\n",
    "x_data = [1, 2, 3, 4, 5]\n",
    "y_data = [1, 2, 3, 4, 5]\n",
    "\n",
    "# W, b initialize\n",
    "W = tf.Variable(2.9)\n",
    "b = tf.Variable(0.5)\n",
    "\n",
    "# W, b update\n",
    "for i in range(100):\n",
    "    # Gradient descent\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = W * x_data + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    if i % 10 == 0:\n",
    "        print(\"{:5}|{:10.4f}|{:10.4f}|{:10.6f}\".format(i, W.numpy(), b.numpy(), cost))\n",
    "\n",
    "print()\n",
    "\n",
    "# predict\n",
    "print(W * 5 + b)\n",
    "print(W * 2.5 + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-curve",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-enclosure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
